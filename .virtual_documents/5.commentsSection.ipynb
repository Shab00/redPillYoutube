from google.colab import drive
drive.mount('/content/drive')


get_ipython().run_line_magic("load_ext", " cudf.pandas")











! pip install vaderSentiment pyLDAvis


import pandas as pd
import numpy as np
import re
from datetime import datetime
from dateutil import parser
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from wordcloud import WordCloud, STOPWORDS
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
import pyLDAvis
import xgboost as xgb
import seaborn as sns
sns.set(style="darkgrid", color_codes=True)
from collections import Counter
from tqdm import tqdm
from tqdm.notebook import tqdm
import transformers
print(transformers.__version__)

from collections import Counter


from transformers import pipeline, AutoTokenizer
import pandas as pd
from tqdm.notebook import tqdm


import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, pipeline
import torch
from torch.utils.data import Dataset


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')














comments_df = pd.read_csv("/content/drive/MyDrive/repository/dataFolder/processed/cleanedComments.csv")


video_df = pd.read_csv("/content/drive/MyDrive/repository/dataFolder/processed/cleanedDataFrame.csv")


video_df.columns


comments_df.columns





merged_df = comments_df.merge(video_df[['video_id', 'title']], on='video_id', how='left')
print(merged_df.head())





merged_df














analyzer = SentimentIntensityAnalyzer()








merged_df['vader_sentiment'] = merged_df['comment'].apply(
    lambda x: analyzer.polarity_scores(str(x))['compound'] if isinstance(x, str) else np.nan
)








merged_df['vader_sentiment_label'] = merged_df['vader_sentiment'].apply(
    lambda score: 'Positive' if score > 0 else 'Negative'
)








merged_df['textblob_sentiment'] = merged_df['comment'].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)








merged_df['textblob_sentiment_label'] = merged_df['textblob_sentiment'].apply(
    lambda score: 'Positive' if score > 0 else 'Negative'
)








print(merged_df['vader_sentiment_label'].value_counts())
print(merged_df['textblob_sentiment_label'].value_counts())

















sns.countplot(data=merged_df, x='vader_sentiment_label', hue='vader_sentiment_label',
              order=['Positive', 'Negative'], palette=["#4ECDC4", "#FF6B6B"], legend=False)
plt.title("Distribution of Sentiments (VADER)")
plt.show()





sns.countplot(data=merged_df, x='textblob_sentiment_label', hue='textblob_sentiment_label',
              order=['Positive', 'Negative'], palette=["#4ECDC4", "#FF6B6B"], legend=False)
plt.title("Distribution of Sentiments (TextBlob)")
plt.show()














def print_comments_neatly(df, sentiment_label, num_comments=5):
    print(f"\n{'='*20} {sentiment_label.upper()} COMMENTS {'='*20}")
    filtered_df = df[df['vader_sentiment_label'] == sentiment_label]
    for i, row in enumerate(filtered_df.itertuples(), 1):
        print(f"{i}. Comment: {row.comment}")
        print(f"   Sentiment Score: {row.vader_sentiment:.2f}")
        print("-" * 60)
        if i >= num_comments:
            break

print_comments_neatly(merged_df, sentiment_label='Positive', num_comments=5)

print_comments_neatly(merged_df, sentiment_label='Negative', num_comments=5)














sns.histplot(merged_df['vader_sentiment'], bins=30, kde=True, color='#4ECDC4')
plt.title('Distribution of VADER Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()











merged_df['comment_length'] = merged_df['comment'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)

plt.figure(figsize=(10, 6))
sns.scatterplot(data=merged_df, x='vader_sentiment', y='comment_length', alpha=0.5, color='purple')
plt.title('Scatter Plot: Sentiment Score vs. Comment Length')
plt.xlabel('VADER Sentiment Score')
plt.ylabel('Comment Length (Number of Characters)')
plt.show()














def get_most_frequent_words(df, sentiment_filter, num_words=10):
    filtered_comments = df[df['vader_sentiment'].apply(sentiment_filter)]['comment']

    all_words = ' '.join(str(comment) for comment in filtered_comments).lower().split()
    cleaned_words = [word for word in all_words if word not in STOPWORDS and word.isalpha()]

    most_common_words = Counter(cleaned_words).most_common(num_words)
    return pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])

positive_words_df = get_most_frequent_words(merged_df, sentiment_filter=lambda x: x > 0.9)

plt.figure(figsize=(10, 6))
sns.barplot(data=positive_words_df, x='Frequency', y='Word', hue='Word', dodge=False, palette=['#4ECDC4'] * len(positive_words_df))
plt.title('Most Frequent Words in Highly Positive Comments (VADER Sentiment > 0.9)')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.legend([],[], frameon=False)
plt.show()

negative_words_df = get_most_frequent_words(merged_df, sentiment_filter=lambda x: x < -0.9)

plt.figure(figsize=(10, 6))
sns.barplot(data=negative_words_df, x='Frequency', y='Word', hue='Word', dodge=False, palette=['#FF6B6B'] * len(negative_words_df))
plt.title('Most Frequent Words in Highly Negative Comments (VADER Sentiment < -0.9)')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.legend([],[], frameon=False)
plt.show()




















def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)








merged_df['cleaned_comment'] = merged_df['comment'].dropna().astype(str).apply(preprocess_text)











def perform_topic_modeling(df, num_topics=5, num_words=10):
    comments = df['cleaned_comment'].dropna().astype(str)

    vectorizer = CountVectorizer(stop_words='english', max_features=5000)
    dtm = vectorizer.fit_transform(comments)

    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(dtm)

    topics = []
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(lda.components_):
        top_words = [feature_names[i] for i in topic.argsort()[-num_words:][::-1]]
        topics.append(f"Topic {topic_idx + 1}: " + ", ".join(top_words))

    return lda, dtm, vectorizer, topics

lda_model, dtm, vectorizer, topic_descriptions = perform_topic_modeling(merged_df, num_topics=5, num_words=10)

for topic in topic_descriptions:
    print(topic)

pyLDAvis.enable_notebook()

lda_vis = pyLDAvis.prepare(topic_term_dists=lda_model.components_,
                           doc_topic_dists=lda_model.transform(dtm),
                           doc_lengths=dtm.sum(axis=1).A1,
                           vocab=vectorizer.get_feature_names_out(),
                           term_frequency=np.asarray(dtm.sum(axis=0)).ravel())

pyLDAvis.display(lda_vis)








doc_topic_matrix = lda_model.transform(dtm)
print(doc_topic_matrix)


topic_word_matrix = lda_model.components_
print(topic_word_matrix)


doc_topic_df = pd.DataFrame(doc_topic_matrix, columns=[f"Topic {i+1}" for i in range(lda_model.n_components)])
print(doc_topic_df.head())


feature_names = vectorizer.get_feature_names_out()
topic_word_df = pd.DataFrame(topic_word_matrix, columns=feature_names)
print(topic_word_df.head())








def display_topics_table(lda_model, vectorizer, num_words=10):
    feature_names = vectorizer.get_feature_names_out()
    topic_dict = {}
    for topic_idx, topic in enumerate(lda_model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[-num_words:][::-1]]
        topic_dict[f"Topic {topic_idx + 1}"] = top_words
    topics_df = pd.DataFrame(topic_dict)
    print(topics_df)
    return topics_df

topics_df = display_topics_table(lda_model, vectorizer, num_words=10)





doc_topic_matrix = lda_model.transform(dtm)
topic_counts = np.argmax(doc_topic_matrix, axis=1)
topic_distribution = pd.Series(topic_counts).value_counts().sort_index()

plt.figure(figsize=(8,5))
plt.bar([f"Topic {i+1}" for i in topic_distribution.index], topic_distribution.values)
plt.xlabel("Topics")
plt.ylabel("Number of Documents")
plt.title("Distribution of Topics in Corpus")
plt.show()

















print("VADER label counts:\n", merged_df['vader_sentiment_label'].value_counts())
print("TextBlob label counts:\n", merged_df['textblob_sentiment_label'].value_counts())

agreement = (merged_df['vader_sentiment_label'] == merged_df['textblob_sentiment_label']).mean()
print(f"Agreement between VADER and TextBlob: {agreement:.2%}")








doc_topic_matrix = lda_model.transform(dtm)


X = doc_topic_matrix

y_vader = merged_df.loc[comments_df.index, 'vader_sentiment_label'].reset_index(drop=True)
y_textblob = merged_df.loc[comments_df.index, 'textblob_sentiment_label'].reset_index(drop=True)








print(X.shape)
print(y_vader.shape)
print(y_textblob.shape)





missing_indices = set(comments_df.index) - set(merged_df.index[:X.shape[0]])
print(missing_indices)


missing_indices = {1079840, 1079841, 1079842, 1079843, 1079844, 1079845, 1079836, 1079837, 1079838, 1079839}

filtered_comments_df = comments_df.drop(missing_indices)

y_vader = merged_df.loc[filtered_comments_df.index, 'vader_sentiment_label']
y_textblob = merged_df.loc[filtered_comments_df.index, 'textblob_sentiment_label']

print(X.shape)
print(y_vader.shape)
print(y_textblob.shape)








def run_sentiment_model(X, y, label_source):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    clf = LogisticRegression(max_iter=1000, random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(f"\nResults for {label_source}:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    return clf, X_test, y_test, y_pred


clf_vader, X_test_vader, y_test_vader, y_pred_vader = run_sentiment_model(X, y_vader, "VADER")
clf_textblob, X_test_textblob, y_test_textblob, y_pred_textblob = run_sentiment_model(X, y_textblob, "TextBlob")














fig, axs = plt.subplots(1, 2, figsize=(14, 5))

sns.heatmap(confusion_matrix(y_test_vader, y_pred_vader), annot=True, fmt='d', cmap='BuGn', ax=axs[0], cbar=False)
axs[0].set_title('VADER Confusion Matrix')
axs[0].set_xlabel('Predicted')
axs[0].set_ylabel('Actual')
axs[0].set_xticklabels(['Negative', 'Positive'])
axs[0].set_yticklabels(['Negative', 'Positive'], rotation=0)

sns.heatmap(confusion_matrix(y_test_textblob, y_pred_textblob), annot=True, fmt='d', cmap='Reds', ax=axs[1], cbar=False)
axs[1].set_title('TextBlob Confusion Matrix')
axs[1].set_xlabel('Predicted')
axs[1].set_ylabel('Actual')
axs[1].set_xticklabels(['Negative', 'Positive'])
axs[1].set_yticklabels(['Negative', 'Positive'], rotation=0)

plt.tight_layout()
plt.show()











vader_report = classification_report(y_test_vader, y_pred_vader, output_dict=True)
textblob_report = classification_report(y_test_textblob, y_pred_textblob, output_dict=True)

vader_df = pd.DataFrame(vader_report).transpose().iloc[:-3, :]
textblob_df = pd.DataFrame(textblob_report).transpose().iloc[:-3, :]

custom_colors = ['#FF6B6B', '#4ECDC4', '#22223B']

fig, axs = plt.subplots(1, 2, figsize=(14, 5))

vader_df[['precision', 'recall', 'f1-score']].plot.bar(
    ax=axs[0],
    ylim=(0,1),
    title='VADER',
    color=custom_colors
)
axs[0].set_ylabel('Score')

textblob_df[['precision', 'recall', 'f1-score']].plot.bar(
    ax=axs[1],
    ylim=(0,1),
    title='TextBlob',
    color=custom_colors
)

plt.tight_layout()
plt.show()








fig, axs = plt.subplots(1, 2, figsize=(12,5))

vader_counts = pd.Series(y_pred_vader).value_counts().sort_index()
textblob_counts = pd.Series(y_pred_textblob).value_counts().sort_index()

sentiment_palette = {'Negative': '#FF6B6B', 'Positive': '#4ECDC4'}

vader_colors = [sentiment_palette[label] for label in vader_counts.index]
vader_counts.plot(
    kind='bar',
    ax=axs[0],
    title='VADER Predicted Sentiment',
    color=vader_colors
)

textblob_colors = [sentiment_palette[label] for label in textblob_counts.index]
textblob_counts.plot(
    kind='bar',
    ax=axs[1],
    title='TextBlob Predicted Sentiment',
    color=textblob_colors
)

plt.tight_layout()
plt.show()








labels = sorted(y_test_vader.unique())
x = range(len(labels))

sentiment_palette = {'Negative': '#FF6B6B', 'Positive': '#4ECDC4'}
bar_colors = [sentiment_palette[label] for label in labels]

fig, axs = plt.subplots(1, 2, figsize=(14, 5))

axs[0].bar(x, pd.Series(y_test_vader).value_counts().sort_index(), alpha=0.5, label='Actual', color=bar_colors)
axs[0].bar(x, pd.Series(y_pred_vader).value_counts().sort_index(), alpha=0.5, label='Predicted', color=bar_colors, hatch='//')
axs[0].set_xticks(x)
axs[0].set_xticklabels(labels)
axs[0].set_title('VADER: Actual vs Predicted')
axs[0].legend()

axs[1].bar(x, pd.Series(y_test_textblob).value_counts().sort_index(), alpha=0.5, label='Actual', color=bar_colors)
axs[1].bar(x, pd.Series(y_pred_textblob).value_counts().sort_index(), alpha=0.5, label='Predicted', color=bar_colors, hatch='//')
axs[1].set_xticks(x)
axs[1].set_xticklabels(labels)
axs[1].set_title('TextBlob: Actual vs Predicted')
axs[1].legend()

plt.tight_layout()
plt.show()

















label_column = 'vader_sentiment_label'

label_map = {'Negative': 0, 'Positive': 1}
merged_df['label'] = merged_df[label_column].map(label_map)


X = merged_df['cleaned_comment'].astype(str).tolist()
y = merged_df['label'].tolist()

train_texts, test_texts, train_labels, test_labels = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)








tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_texts(texts):
    return tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

train_encodings = tokenize_texts(train_texts)
test_encodings = tokenize_texts(test_texts)








class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    def __len__(self):
        return len(self.labels)

train_dataset = SentimentDataset(train_encodings, train_labels)
test_dataset = SentimentDataset(test_encodings, test_labels)





model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)








training_args = TrainingArguments(
    output_dir='./results',
    report_to="none",
    num_train_epochs=2,  # You can increase if you have time/resources
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    eval_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    logging_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model='eval_accuracy',
    save_total_limit=1,
)





def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    acc = (labels == preds).mean()
    return {'accuracy': acc}





trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)





trainer.train()








save_path = "/content/drive/MyDrive/my_bert_model"
trainer.save_model(save_path)

tokenizer.save_pretrained(save_path)





classifier = pipeline("text-classification", model="./results/final")











classifier = pipeline("text-classification", model="./results/final")

test_comments = [
    "What color is your Bugatti?",
    "Action is the only way you’ll progress. Not talking. Not planning. And not reading books.",
    "Moody females steal your power. It’s dangerous for a man. A man must remain focused.",
    "They went through all that shit to raise you, and now you're sad so you jump off a fucking bridge. If my kid did that, I'd be pissed. What a fucking moron! I wouldn't even give him a funeral",
    "Your mind must be stronger than your feelings."
]

label_map = {
    "LABEL_0": "Negative",
    "LABEL_1": "Positive"
}

for comment in test_comments:
    pred = classifier(comment)[0]
    print(f"Text: {comment}")
    print(f"Prediction: {label_map[pred['label']]} (score: {pred['score']:.2f})\n")











batch_size = 64
preds = []
for i in tqdm(range(0, len(test_texts), batch_size), desc="Batch prediction"):
    batch = test_texts[i:i+batch_size]
    batch_preds = classifier(batch, truncation=True)
    preds.extend([p['label'] for p in batch_preds])











cm = np.array([[98127, 8649], [11547, 97647]])
labels = ["Negative", "Positive"]

cm_sum = np.sum(cm)
cm_percent = cm / cm_sum * 100

annot = np.empty_like(cm).astype(str)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        annot[i, j] = f"{cm[i, j]}\n({cm_percent[i, j]:.1f}%)"

plt.figure(figsize=(7,6))
ax = sns.heatmap(
    cm,
    annot=annot,
    fmt='',
    cmap=sns.color_palette(["#FF6B6B", "#4ECDC4"], as_cmap=True),
    cbar=False,
    linewidths=2,
    linecolor='white',
    square=True,
    annot_kws={"fontsize":16, "weight":'bold'}
)

ax.set_xlabel('Predicted Label', fontsize=16, weight='bold')
ax.set_ylabel('True Label', fontsize=16, weight='bold')
ax.set_xticklabels(labels, fontsize=14, weight='bold')
ax.set_yticklabels(labels, fontsize=14, weight='bold', rotation=0)
ax.set_title('Confusion Matrix - Test Set', fontsize=18, weight='bold', pad=20)
plt.tight_layout()
plt.show()


print("Confusion Matrix:")
print("           Predicted Negative  Predicted Positive")
print(f"Actual Negative      {cm[0,0]}                 {cm[0,1]}")
print(f"Actual Positive      {cm[1,0]}                 {cm[1,1]}")











enriched_comments_df = comments_df.merge(
    video_df[['video_id', 'title', 'publishedAt']],
    on='video_id',
    how='left'
)


enriched_comments_df.columns


emotion_classifier = pipeline(
    "text-classification",
    model="bhadresh-savani/distilbert-base-uncased-emotion",
    return_all_scores=False
)


enriched_comments_df['comment_length'] = enriched_comments_df['comment'].str.len()
print((enriched_comments_df['comment_length'] > 1000).sum())


enriched_comments_df['was_truncated'] = enriched_comments_df['comment'].str.len() > 1000


def safe_predict_emotion(text, max_chars=1000):
    short_text = str(text)[:max_chars]
    try:
        return emotion_classifier(short_text)[0]['label']
    except Exception as e:
        return "error"


tqdm.pandas()
enriched_comments_df['emotion'] = enriched_comments_df['comment'].progress_apply(safe_predict_emotion)





print(enriched_comments_df[['comment', 'emotion']].sample(5))


video_emotions = enriched_comments_df.groupby('title')['emotion'].agg(lambda x: x.mode()[0])
print(video_emotions.head())


enriched_comments_df.to_csv('comments_with_emotion.csv', index=False)


print(emotion_classifier("Joe Biden"))
print(emotion_classifier("I love this guy (even though he’s a Trump supporter)"))








classifier = pipeline(
    task="text-classification",
    model="SamLowe/roberta-base-go_emotions",
    top_k=None,
    truncation=True,
    device=0
)
tokenizer = AutoTokenizer.from_pretrained("SamLowe/roberta-base-go_emotions")

def get_top_emotions(text, threshold=0.2, top_n=3, max_tokens=512):
    try:
        tokens = tokenizer.encode(str(text), truncation=True, max_length=max_tokens, return_tensors=None)
        short_text = tokenizer.decode(tokens, skip_special_tokens=True)
        result = classifier(short_text)[0]
        filtered = [d for d in result if d['label'] != 'neutral' and d['score'] > threshold]
        filtered = sorted(filtered, key=lambda d: d['score'], reverse=True)
        if not filtered:
            filtered = [d for d in result if d['label'] == 'neutral']
        return ', '.join(f"{d['label']} ({d['score']:.2f})" for d in filtered[:top_n])
    except Exception as e:
        return 'error'


tqdm.pandas()
enriched_comments_df['emotions'] = enriched_comments_df['comment'].progress_apply(get_top_emotions)





print(enriched_comments_df[['comment', 'emotions']].head(10))





enriched_comments_df.to_csv("comments_with_emotions_SamLowe2.csv", index=False)





enriched_comments_df['top_emotion'] = enriched_comments_df['emotions'].str.extract(r"^(\w+)")

plt.figure(figsize=(12,6))
sns.countplot(y='top_emotion', data=enriched_comments_df, order=enriched_comments_df['top_emotion'].value_counts().index)
plt.title("Most Common Top Emotions in Comments")
plt.xlabel("Count")
plt.ylabel("Emotion")
plt.tight_layout()
plt.show()


top_emotions = enriched_comments_df['top_emotion'].value_counts().head(8)
plt.figure(figsize=(8,8))
top_emotions.plot.pie(autopct='%1.1f%%', startangle=90)
plt.title("Top 8 Emotions Pie Chart")
plt.ylabel("")
plt.show()


def extract_all_emotions(text):
    if pd.isnull(text):
        return []
    return [e.strip().split(' (')[0] for e in text.split(',')]

all_emotions = enriched_comments_df['emotions'].dropna().apply(extract_all_emotions)
flat_emotions = [item for sublist in all_emotions for item in sublist]
emotion_counts = pd.Series(Counter(flat_emotions))

plt.figure(figsize=(12,6))
emotion_counts.sort_values(ascending=False).plot.bar()
plt.title("Distribution of All Mentioned Emotions")
plt.xlabel("Emotion")
plt.ylabel("Count")
plt.tight_layout()
plt.show()





enriched_comments_df = pd.read_csv("/content/comments_with_emotions_SamLowe2.csv", low_memory=False)








print(enriched_comments_df.shape)
print(enriched_comments_df.columns)
print(enriched_comments_df.head())
print(enriched_comments_df['emotions'].value_counts().head(20))








enriched_comments_df['top_emotion'] = enriched_comments_df['emotions'].str.extract(r"^(\w+)")
emotion_counts = enriched_comments_df['top_emotion'].value_counts()

emotion_counts.plot.bar(figsize=(12,6), title='Distribution of Top Emotions')
plt.xlabel('Emotion')
plt.ylabel('Count')
plt.show()








if 'video' in enriched_comments_df.columns:
    import seaborn as sns
    plt.figure(figsize=(16,8))
    sns.countplot(y='top_emotion', hue='video', data=enriched_comments_df)
    plt.title('Emotion Distribution per Video')
    plt.show()





non_neutral_df = enriched_comments_df[enriched_comments_df['top_emotion'] != 'neutral']


print(non_neutral_df[non_neutral_df['top_emotion'] == 'approval']['comment'].sample(5))








emotional_df = enriched_comments_df[enriched_comments_df['top_emotion'] != 'neutral'].copy()


video_emotion_counts = emotional_df['title'].value_counts().head(10)
print(video_emotion_counts)








happy_emotions = ['joy', 'amusement', 'love', 'gratitude', 'excitement', 'optimism']
angry_emotions = ['anger', 'annoyance']

happy_counts = emotional_df[emotional_df['top_emotion'].isin(happy_emotions)].groupby('title').size()
angry_counts = emotional_df[emotional_df['top_emotion'].isin(angry_emotions)].groupby('title').size()

print("Videos with most happy comments:\n", happy_counts.sort_values(ascending=False).head(5))
print("Videos with most angry comments:\n", angry_counts.sort_values(ascending=False).head(5))








top_emotions_per_video = emotional_df.groupby(['title', 'top_emotion']).size().unstack(fill_value=0)
top3_emotions = top_emotions_per_video.apply(lambda row: row.sort_values(ascending=False).head(3), axis=1)
print(top3_emotions)








total_comments = len(enriched_comments_df)
neutral_count = (enriched_comments_df['top_emotion'] == 'neutral').sum()
emotional_count = total_comments - neutral_count

print(f"Total comments: {total_comments}")
print(f"Neutral: {neutral_count} ({neutral_count/total_comments:.2%})")
print(f"Emotional: {emotional_count} ({emotional_count/total_comments:.2%})")





plt.bar(['Neutral', 'Emotional'], [neutral_count, emotional_count], color=['gray', 'orange'])
plt.title('Neutral vs Emotional Comments')
plt.ylabel('Count')
plt.show()








emotions = emotional_df['top_emotion'].unique()

for emotion in emotions:
    text = ' '.join(emotional_df[emotional_df['top_emotion'] == emotion]['comment'].astype(str))
    if not text.strip():
        continue
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10,5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud: {emotion.capitalize()}')
    plt.show()








positive_emotions = [
    'joy', 'amusement', 'love', 'gratitude', 'excitement', 'optimism',
    'approval', 'admiration', 'relief', 'pride', 'caring'
]
negative_emotions = [
    'anger', 'annoyance', 'disapproval', 'disappointment', 'sadness', 'fear',
    'disgust', 'remorse', 'nervousness', 'embarrassment'
]

positive_text = ' '.join(emotional_df[emotional_df['top_emotion'].isin(positive_emotions)]['comment'].astype(str))
negative_text = ' '.join(emotional_df[emotional_df['top_emotion'].isin(negative_emotions)]['comment'].astype(str))

positive_wc = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
negative_wc = WordCloud(width=800, height=400, background_color='white').generate(negative_text)

fig, axs = plt.subplots(1, 2, figsize=(20, 8))

axs[0].imshow(positive_wc, interpolation='bilinear')
axs[0].axis('off')
axs[0].set_title('Word Cloud: Positive Emotions', fontsize=20)

axs[1].imshow(negative_wc, interpolation='bilinear')
axs[1].axis('off')
axs[1].set_title('Word Cloud: Negative Emotions', fontsize=20)

plt.tight_layout()
plt.show()








happy_emotions = ['joy', 'amusement', 'love', 'gratitude', 'excitement', 'optimism']
angry_emotions = ['anger', 'annoyance']
target_emotions = happy_emotions + angry_emotions

filtered = emotional_df[emotional_df['top_emotion'].isin(target_emotions)]

top_videos = (
    filtered['title']
    .value_counts()
    .head(20)
    .index
)
filtered = filtered[filtered['title'].isin(top_videos)]

filtered_pivot = (
    filtered
    .pivot_table(index='title', columns='top_emotion', values='comment', aggfunc='count', fill_value=0)
)

log_pivot = np.log1p(filtered_pivot)

plt.figure(figsize=(12, 8))
sns.heatmap(log_pivot, cmap='YlOrRd', annot=True, fmt=".2f")
plt.title('Log-Scaled Happy/Angry Emotion Counts by Top 20 Videos')
plt.xlabel('Emotion')
plt.ylabel('Video Title')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()








filtered_pivot['total'] = filtered_pivot.sum(axis=1)
top_video_idx = filtered_pivot['total'].idxmax()
top_video_row = filtered_pivot.loc[top_video_idx]
print(f"Video with the most happy/angry comments:\nIndex: {top_video_idx}\nCounts:\n{top_video_row}")

if isinstance(top_video_idx, tuple):
    video_title, channel_title = top_video_idx
    print(f"\nTitle: {video_title}\nChannel: {channel_title}")
else:
    print(f"\nVideo identifier: {top_video_idx}")


optimism_comments = emotional_df[
    (emotional_df['title'] == "Truth as the Antidote to Suffering (with Lewis Howes)") &
    (emotional_df['top_emotion'] == "optimism")
]['comment']

print("Sample optimism comments:\n", optimism_comments.sample(20, random_state=42).to_list())








filtered_emotional_df = emotional_df[emotional_df['title'] != "Truth as the Antidote to Suffering (with Lewis Howes)"]


happy_emotions = ['joy', 'amusement', 'love', 'gratitude', 'excitement', 'optimism']
angry_emotions = ['anger', 'annoyance']

dummies = pd.get_dummies(filtered_emotional_df['top_emotion'])
df_with_dummies = pd.concat([filtered_emotional_df, dummies], axis=1)

filtered_pivot = (
    df_with_dummies
    .groupby(['title', 'channelTitle'])[happy_emotions + angry_emotions]
    .sum()
    .fillna(0)
)

filtered_pivot['total'] = filtered_pivot.sum(axis=1)
filtered_pivot = filtered_pivot.sort_values('total', ascending=False).head(20)
filtered_pivot = filtered_pivot.drop(columns='total')

log_pivot = np.log1p(filtered_pivot)

plt.figure(figsize=(14,8))
sns.heatmap(log_pivot, cmap='YlOrRd', annot=True, fmt=".2f")
plt.title('Log-Scaled Happy/Angry Emotion Counts by Top Videos (Outlier Excluded)')
plt.xlabel('Emotion')
plt.ylabel('Video (Title, Channel)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()








filtered_emotional_df = filtered_emotional_df.copy()
filtered_emotional_df['published_dt'] = pd.to_datetime(filtered_emotional_df['published_at'])

timeline = (
    filtered_emotional_df
    .set_index('published_dt')
    .groupby([pd.Grouper(freq='D'), 'top_emotion'])
    .size()
    .unstack(fill_value=0)
)

timeline[happy_emotions + angry_emotions].plot(figsize=(16,6), title='Emotions Over Time')
plt.ylabel('Count')
plt.show()


for emotion in happy_emotions + angry_emotions:
    timeline[emotion].plot(figsize=(14, 4), title=f'Emotion Over Time: {emotion}')
    plt.ylabel('Count')
    plt.xlabel('Date')
    plt.show()








timeline['positive'] = timeline[happy_emotions].sum(axis=1)
timeline['negative'] = timeline[angry_emotions].sum(axis=1)

ax = timeline['positive'].plot(figsize=(16,6), color='green', label='Positive')
timeline['negative'].plot(ax=ax, color='red', label='Negative')
plt.title('Positive and Negative Emotions Over Time')
plt.ylabel('Count')
plt.xlabel('Date')
plt.legend()
plt.show()








if 'channelTitle' in emotional_df.columns:
    top_channels = emotional_df['channelTitle'].value_counts().head(10)
    print("Top channels with emotional comments:\n", top_channels)
    top_channels.plot.bar(title='Top Channels (Emotional Comments)', figsize=(10,5))
    plt.ylabel('Number of Emotional Comments')
    plt.xlabel('Channel')
    plt.show()








diversity = emotional_df.groupby(['title', 'channelTitle'])['top_emotion'].nunique()
most_diverse = diversity.sort_values(ascending=False).head(10)
print("Videos with the most diverse emotions (Title, Channel):\n", most_diverse)

most_diverse_video = most_diverse.index[0]
video_title, channel_title = most_diverse_video

video_emotions = emotional_df[
    (emotional_df['title'] == video_title) &
    (emotional_df['channelTitle'] == channel_title)
]['top_emotion'].value_counts()

print(f"\nTop emotions for video '{video_title}' by channel '{channel_title}':\n", video_emotions)








positive_emotions = ['joy', 'amusement', 'love', 'gratitude', 'excitement', 'optimism', 'approval', 'admiration', 'relief', 'pride', 'caring']
negative_emotions = ['anger', 'annoyance', 'disapproval', 'disappointment', 'sadness', 'fear', 'disgust', 'remorse', 'nervousness', 'embarrassment']

def sentiment_label(emotion):
    if emotion in positive_emotions:
        return 'positive'
    elif emotion in negative_emotions:
        return 'negative'
    else:
        return 'other'

emotional_df['sentiment'] = emotional_df['top_emotion'].apply(sentiment_label)

sentiment_counts = emotional_df['sentiment'].value_counts()
print("Overall sentiment summary:\n", sentiment_counts)

sentiment_by_video = emotional_df.groupby('title')['sentiment'].value_counts().unstack(fill_value=0)
print("Sentiment breakdown by video:\n", sentiment_by_video.head())








sentiment_by_video_channel = emotional_df.groupby(['title', 'channelTitle'])['sentiment'].value_counts().unstack(fill_value=0)
print("Sentiment breakdown by video and channel:\n", sentiment_by_video_channel.head())

sentiment_by_video_channel_reset = sentiment_by_video_channel.reset_index()
print(sentiment_by_video_channel_reset.head())








enriched_comments_df['sentiment'] = enriched_comments_df['top_emotion'].apply(
    lambda x: 'neutral' if x == 'neutral' else sentiment_label(x)
)
full_sentiment_counts = enriched_comments_df['sentiment'].value_counts()
print("Full sentiment breakdown (including neutral):\n", full_sentiment_counts)

full_sentiment_counts.plot.pie(autopct='%1.1f%%', title='Sentiment Distribution')
plt.ylabel('')
plt.show()








enriched_comments_df = pd.read_csv("/content/comments_with_emotions_SamLowe2.csv", low_memory=False)


enriched_comments_df.columns


enriched_comments_df['vader_sentiment'] = enriched_comments_df['comment'].apply(
    lambda x: analyzer.polarity_scores(str(x))['compound'] if isinstance(x, str) else np.nan
)


enriched_comments_df['vader_sentiment_label'] = enriched_comments_df['vader_sentiment'].apply(
    lambda score: 'Positive' if score > 0 else 'Negative'
)


print(enriched_comments_df['channelTitle'].unique()[:20])


timestamp_regex = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$'
timestamp_rows = enriched_comments_df[enriched_comments_df['channelTitle'].str.match(timestamp_regex)]

# 2. Count unique video_ids for each "timestamp channel"
timestamp_video_counts = (
    timestamp_rows.groupby('channelTitle')['video_id']
    .nunique()
    .sort_values(ascending=False)
)

print(timestamp_video_counts)



# 1. Remove channelTitles that are timestamps
timestamp_regex = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$'
filtered_df = enriched_comments_df[~enriched_comments_df['channelTitle'].str.match(timestamp_regex)]

# 2. Remove rows with 'neutral' top_emotion (case-insensitive)
filtered_df = filtered_df[filtered_df['top_emotion'].str.lower() != 'neutral']

# 3. Pivot table: count of comments by channel and emotion
pivot = filtered_df.pivot_table(
    index='channelTitle',
    columns='top_emotion',
    values='comment',
    aggfunc='count',
    fill_value=0
)

plt.figure(figsize=(12, 6))
sns.heatmap(pivot, cmap='viridis')
plt.title('Heatmap: Emotion Distribution by Channel (No Timestamps/Neutral)')
plt.xlabel('Emotion')
plt.ylabel('Channel')
plt.tight_layout()
plt.show()


def remove_emojis(text):
    # This regex matches most common emojis
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F1E0-\U0001F1FF"  # flags (iOS)
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )
    return emoji_pattern.sub(r'', str(text))

# Remove emojis from the title column
filtered_df['title'] = filtered_df['title'].apply(remove_emojis)


# Step 1: Find top N titles by comment count
video_comment_counts = filtered_df.groupby('title')['comment'].count()
top_n = 10
top_titles = video_comment_counts.nlargest(top_n).index

# Step 2: Filter your DataFrame for these titles
df_top = filtered_df[filtered_df['title'].isin(top_titles)]

# Step 3: Create the pivot table (now guaranteed to have only the top titles)
pivot_top = df_top.pivot_table(
    index='title',
    columns='vader_sentiment_label',
    values='comment',
    aggfunc='count',
    fill_value=0
)

# Step 4: Reindex to ensure the order matches top_titles
pivot_top = pivot_top.reindex(top_titles)

# Step 5: Plot
ax = pivot_top.plot(kind='bar', stacked=True, figsize=(14,7))
plt.title(f'Sentiment Distribution by Top {top_n} Video Titles')
plt.xlabel('Video Title')
plt.ylabel('Number of Comments')
plt.legend(title='Sentiment')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()




ax = pivot_top.plot(kind='bar', stacked=True, figsize=(14,7))
plt.title(f'Sentiment Distribution by Top {top_n} Video Titles')
plt.xlabel('Video Title')
plt.ylabel('Number of Comments')
plt.legend(title='Sentiment')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


examples = filtered_df[['comment', 'title', 'channelTitle', 'top_emotion', 'vader_sentiment_label']].sample(5)
print("Example Comments with Emotion and Sentiment Breakdown:\n")
print(examples.to_markdown(index=False))






