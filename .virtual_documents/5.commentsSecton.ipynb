from google.colab import drive
drive.mount('/content/drive')











! pip install vaderSentiment pyLDAvis


import pandas as pd
import numpy as np
import re
from datetime import datetime
from dateutil import parser
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, CountVectorizer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from wordcloud import WordCloud, STOPWORDS
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
import pyLDAvis
import xgboost as xgb
import seaborn as sns
sns.set(style="darkgrid", color_codes=True)
from collections import Counter
from tqdm import tqdm

import transformers
print(transformers.__version__)

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, pipeline
import torch
from torch.utils.data import Dataset


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')














comments_df = pd.read_csv("/content/drive/MyDrive/repository/dataFolder/processed/cleanedComments.csv")


video_df = pd.read_csv("/content/drive/MyDrive/repository/dataFolder/processed/cleanedDataFrame.csv")


video_df.columns


comments_df.columns





merged_df = comments_df.merge(video_df[['video_id', 'title']], on='video_id', how='left')
print(merged_df.head())





merged_df














analyzer = SentimentIntensityAnalyzer()








merged_df['vader_sentiment'] = merged_df['comment'].apply(
    lambda x: analyzer.polarity_scores(str(x))['compound'] if isinstance(x, str) else np.nan
)








merged_df['vader_sentiment_label'] = merged_df['vader_sentiment'].apply(
    lambda score: 'Positive' if score > 0 else 'Negative'
)








merged_df['textblob_sentiment'] = merged_df['comment'].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)








merged_df['textblob_sentiment_label'] = merged_df['textblob_sentiment'].apply(
    lambda score: 'Positive' if score > 0 else 'Negative'
)








print(merged_df['vader_sentiment_label'].value_counts())
print(merged_df['textblob_sentiment_label'].value_counts())

















sns.countplot(data=merged_df, x='vader_sentiment_label', hue='vader_sentiment_label',
              order=['Positive', 'Negative'], palette=["#4ECDC4", "#FF6B6B"], legend=False)
plt.title("Distribution of Sentiments (VADER)")
plt.show()





sns.countplot(data=merged_df, x='textblob_sentiment_label', hue='textblob_sentiment_label',
              order=['Positive', 'Negative'], palette=["#4ECDC4", "#FF6B6B"], legend=False)
plt.title("Distribution of Sentiments (TextBlob)")
plt.show()














def print_comments_neatly(df, sentiment_label, num_comments=5):
    print(f"\n{'='*20} {sentiment_label.upper()} COMMENTS {'='*20}")
    filtered_df = df[df['vader_sentiment_label'] == sentiment_label]
    for i, row in enumerate(filtered_df.itertuples(), 1):
        print(f"{i}. Comment: {row.comment}")
        print(f"   Sentiment Score: {row.vader_sentiment:.2f}")
        print("-" * 60)
        if i >= num_comments:
            break

print_comments_neatly(merged_df, sentiment_label='Positive', num_comments=5)

print_comments_neatly(merged_df, sentiment_label='Negative', num_comments=5)














sns.histplot(merged_df['vader_sentiment'], bins=30, kde=True, color='#4ECDC4')
plt.title('Distribution of VADER Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()











merged_df['comment_length'] = merged_df['comment'].apply(lambda x: len(str(x)) if isinstance(x, str) else 0)

plt.figure(figsize=(10, 6))
sns.scatterplot(data=merged_df, x='vader_sentiment', y='comment_length', alpha=0.5, color='purple')
plt.title('Scatter Plot: Sentiment Score vs. Comment Length')
plt.xlabel('VADER Sentiment Score')
plt.ylabel('Comment Length (Number of Characters)')
plt.show()














def get_most_frequent_words(df, sentiment_filter, num_words=10):
    filtered_comments = df[df['vader_sentiment'].apply(sentiment_filter)]['comment']

    all_words = ' '.join(str(comment) for comment in filtered_comments).lower().split()
    cleaned_words = [word for word in all_words if word not in STOPWORDS and word.isalpha()]

    most_common_words = Counter(cleaned_words).most_common(num_words)
    return pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])

positive_words_df = get_most_frequent_words(merged_df, sentiment_filter=lambda x: x > 0.9)

plt.figure(figsize=(10, 6))
sns.barplot(data=positive_words_df, x='Frequency', y='Word', hue='Word', dodge=False, palette=['#4ECDC4'] * len(positive_words_df))
plt.title('Most Frequent Words in Highly Positive Comments (VADER Sentiment > 0.9)')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.legend([],[], frameon=False)
plt.show()

negative_words_df = get_most_frequent_words(merged_df, sentiment_filter=lambda x: x < -0.9)

plt.figure(figsize=(10, 6))
sns.barplot(data=negative_words_df, x='Frequency', y='Word', hue='Word', dodge=False, palette=['#FF6B6B'] * len(negative_words_df))
plt.title('Most Frequent Words in Highly Negative Comments (VADER Sentiment < -0.9)')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.legend([],[], frameon=False)
plt.show()




















def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return ' '.join(tokens)








merged_df['cleaned_comment'] = merged_df['comment'].dropna().astype(str).apply(preprocess_text)











def perform_topic_modeling(df, num_topics=5, num_words=10):
    comments = df['cleaned_comment'].dropna().astype(str)

    vectorizer = CountVectorizer(stop_words='english', max_features=5000)
    dtm = vectorizer.fit_transform(comments)

    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(dtm)

    topics = []
    feature_names = vectorizer.get_feature_names_out()
    for topic_idx, topic in enumerate(lda.components_):
        top_words = [feature_names[i] for i in topic.argsort()[-num_words:][::-1]]
        topics.append(f"Topic {topic_idx + 1}: " + ", ".join(top_words))

    return lda, dtm, vectorizer, topics

lda_model, dtm, vectorizer, topic_descriptions = perform_topic_modeling(merged_df, num_topics=5, num_words=10)

for topic in topic_descriptions:
    print(topic)

pyLDAvis.enable_notebook()

lda_vis = pyLDAvis.prepare(topic_term_dists=lda_model.components_,
                           doc_topic_dists=lda_model.transform(dtm),
                           doc_lengths=dtm.sum(axis=1).A1,
                           vocab=vectorizer.get_feature_names_out(),
                           term_frequency=np.asarray(dtm.sum(axis=0)).ravel())

pyLDAvis.display(lda_vis)








doc_topic_matrix = lda_model.transform(dtm)
print(doc_topic_matrix)


topic_word_matrix = lda_model.components_
print(topic_word_matrix)


doc_topic_df = pd.DataFrame(doc_topic_matrix, columns=[f"Topic {i+1}" for i in range(lda_model.n_components)])
print(doc_topic_df.head())


feature_names = vectorizer.get_feature_names_out()
topic_word_df = pd.DataFrame(topic_word_matrix, columns=feature_names)
print(topic_word_df.head())








def display_topics_table(lda_model, vectorizer, num_words=10):
    feature_names = vectorizer.get_feature_names_out()
    topic_dict = {}
    for topic_idx, topic in enumerate(lda_model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[-num_words:][::-1]]
        topic_dict[f"Topic {topic_idx + 1}"] = top_words
    topics_df = pd.DataFrame(topic_dict)
    print(topics_df)
    return topics_df

topics_df = display_topics_table(lda_model, vectorizer, num_words=10)





doc_topic_matrix = lda_model.transform(dtm)
topic_counts = np.argmax(doc_topic_matrix, axis=1)
topic_distribution = pd.Series(topic_counts).value_counts().sort_index()

plt.figure(figsize=(8,5))
plt.bar([f"Topic {i+1}" for i in topic_distribution.index], topic_distribution.values)
plt.xlabel("Topics")
plt.ylabel("Number of Documents")
plt.title("Distribution of Topics in Corpus")
plt.show()

















print("VADER label counts:\n", merged_df['vader_sentiment_label'].value_counts())
print("TextBlob label counts:\n", merged_df['textblob_sentiment_label'].value_counts())

agreement = (merged_df['vader_sentiment_label'] == merged_df['textblob_sentiment_label']).mean()
print(f"Agreement between VADER and TextBlob: {agreement:.2%}")








doc_topic_matrix = lda_model.transform(dtm)


X = doc_topic_matrix

y_vader = merged_df.loc[comments_df.index, 'vader_sentiment_label'].reset_index(drop=True)
y_textblob = merged_df.loc[comments_df.index, 'textblob_sentiment_label'].reset_index(drop=True)








print(X.shape)
print(y_vader.shape)
print(y_textblob.shape)





missing_indices = set(comments_df.index) - set(merged_df.index[:X.shape[0]])
print(missing_indices)


missing_indices = {1079840, 1079841, 1079842, 1079843, 1079844, 1079845, 1079836, 1079837, 1079838, 1079839}

filtered_comments_df = comments_df.drop(missing_indices)

y_vader = merged_df.loc[filtered_comments_df.index, 'vader_sentiment_label']
y_textblob = merged_df.loc[filtered_comments_df.index, 'textblob_sentiment_label']

print(X.shape)
print(y_vader.shape)
print(y_textblob.shape)








def run_sentiment_model(X, y, label_source):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    clf = LogisticRegression(max_iter=1000, random_state=42)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(f"\nResults for {label_source}:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.2%}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    return clf, X_test, y_test, y_pred


clf_vader, X_test_vader, y_test_vader, y_pred_vader = run_sentiment_model(X, y_vader, "VADER")
clf_textblob, X_test_textblob, y_test_textblob, y_pred_textblob = run_sentiment_model(X, y_textblob, "TextBlob")














fig, axs = plt.subplots(1, 2, figsize=(14, 5))

sns.heatmap(confusion_matrix(y_test_vader, y_pred_vader), annot=True, fmt='d', cmap='BuGn', ax=axs[0], cbar=False)
axs[0].set_title('VADER Confusion Matrix')
axs[0].set_xlabel('Predicted')
axs[0].set_ylabel('Actual')
axs[0].set_xticklabels(['Negative', 'Positive'])
axs[0].set_yticklabels(['Negative', 'Positive'], rotation=0)

sns.heatmap(confusion_matrix(y_test_textblob, y_pred_textblob), annot=True, fmt='d', cmap='Reds', ax=axs[1], cbar=False)
axs[1].set_title('TextBlob Confusion Matrix')
axs[1].set_xlabel('Predicted')
axs[1].set_ylabel('Actual')
axs[1].set_xticklabels(['Negative', 'Positive'])
axs[1].set_yticklabels(['Negative', 'Positive'], rotation=0)

plt.tight_layout()
plt.show()











vader_report = classification_report(y_test_vader, y_pred_vader, output_dict=True)
textblob_report = classification_report(y_test_textblob, y_pred_textblob, output_dict=True)

vader_df = pd.DataFrame(vader_report).transpose().iloc[:-3, :]
textblob_df = pd.DataFrame(textblob_report).transpose().iloc[:-3, :]

custom_colors = ['#FF6B6B', '#4ECDC4', '#22223B']

fig, axs = plt.subplots(1, 2, figsize=(14, 5))

vader_df[['precision', 'recall', 'f1-score']].plot.bar(
    ax=axs[0],
    ylim=(0,1),
    title='VADER',
    color=custom_colors
)
axs[0].set_ylabel('Score')

textblob_df[['precision', 'recall', 'f1-score']].plot.bar(
    ax=axs[1],
    ylim=(0,1),
    title='TextBlob',
    color=custom_colors
)

plt.tight_layout()
plt.show()








fig, axs = plt.subplots(1, 2, figsize=(12,5))

vader_counts = pd.Series(y_pred_vader).value_counts().sort_index()
textblob_counts = pd.Series(y_pred_textblob).value_counts().sort_index()

sentiment_palette = {'Negative': '#FF6B6B', 'Positive': '#4ECDC4'}

vader_colors = [sentiment_palette[label] for label in vader_counts.index]
vader_counts.plot(
    kind='bar',
    ax=axs[0],
    title='VADER Predicted Sentiment',
    color=vader_colors
)

textblob_colors = [sentiment_palette[label] for label in textblob_counts.index]
textblob_counts.plot(
    kind='bar',
    ax=axs[1],
    title='TextBlob Predicted Sentiment',
    color=textblob_colors
)

plt.tight_layout()
plt.show()








labels = sorted(y_test_vader.unique())
x = range(len(labels))

sentiment_palette = {'Negative': '#FF6B6B', 'Positive': '#4ECDC4'}
bar_colors = [sentiment_palette[label] for label in labels]

fig, axs = plt.subplots(1, 2, figsize=(14, 5))

axs[0].bar(x, pd.Series(y_test_vader).value_counts().sort_index(), alpha=0.5, label='Actual', color=bar_colors)
axs[0].bar(x, pd.Series(y_pred_vader).value_counts().sort_index(), alpha=0.5, label='Predicted', color=bar_colors, hatch='//')
axs[0].set_xticks(x)
axs[0].set_xticklabels(labels)
axs[0].set_title('VADER: Actual vs Predicted')
axs[0].legend()

axs[1].bar(x, pd.Series(y_test_textblob).value_counts().sort_index(), alpha=0.5, label='Actual', color=bar_colors)
axs[1].bar(x, pd.Series(y_pred_textblob).value_counts().sort_index(), alpha=0.5, label='Predicted', color=bar_colors, hatch='//')
axs[1].set_xticks(x)
axs[1].set_xticklabels(labels)
axs[1].set_title('TextBlob: Actual vs Predicted')
axs[1].legend()

plt.tight_layout()
plt.show()

















label_column = 'vader_sentiment_label'

label_map = {'Negative': 0, 'Positive': 1}
merged_df['label'] = merged_df[label_column].map(label_map)


X = merged_df['cleaned_comment'].astype(str).tolist()
y = merged_df['label'].tolist()

train_texts, test_texts, train_labels, test_labels = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)








tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_texts(texts):
    return tokenizer(
        texts,
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

train_encodings = tokenize_texts(train_texts)
test_encodings = tokenize_texts(test_texts)








class SentimentDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    def __len__(self):
        return len(self.labels)

train_dataset = SentimentDataset(train_encodings, train_labels)
test_dataset = SentimentDataset(test_encodings, test_labels)





model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)








training_args = TrainingArguments(
    output_dir='./results',
    report_to="none",
    num_train_epochs=2,  # You can increase if you have time/resources
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    eval_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    logging_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model='eval_accuracy',
    save_total_limit=1,
)





def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    acc = (labels == preds).mean()
    return {'accuracy': acc}





trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics,
)





trainer.train()








save_path = "/content/drive/MyDrive/my_bert_model"
trainer.save_model(save_path)

tokenizer.save_pretrained(save_path)





classifier = pipeline("text-classification", model="./results/final")











classifier = pipeline("text-classification", model="./results/final")

test_comments = [
    "What color is your Bugatti?",
    "Action is the only way you’ll progress. Not talking. Not planning. And not reading books.",
    "Moody females steal your power. It’s dangerous for a man. A man must remain focused.",
    "They went through all that shit to raise you, and now you're sad so you jump off a fucking bridge. If my kid did that, I'd be pissed. What a fucking moron! I wouldn't even give him a funeral",
    "Your mind must be stronger than your feelings."
]

label_map = {
    "LABEL_0": "Negative",
    "LABEL_1": "Positive"
}

for comment in test_comments:
    pred = classifier(comment)[0]
    print(f"Text: {comment}")
    print(f"Prediction: {label_map[pred['label']]} (score: {pred['score']:.2f})\n")











batch_size = 64
preds = []
for i in tqdm(range(0, len(test_texts), batch_size), desc="Batch prediction"):
    batch = test_texts[i:i+batch_size]
    batch_preds = classifier(batch, truncation=True)
    preds.extend([p['label'] for p in batch_preds])











cm = np.array([[98127, 8649], [11547, 97647]])
labels = ["Negative", "Positive"]

cm_sum = np.sum(cm)
cm_percent = cm / cm_sum * 100

annot = np.empty_like(cm).astype(str)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        annot[i, j] = f"{cm[i, j]}\n({cm_percent[i, j]:.1f}%)"

plt.figure(figsize=(7,6))
ax = sns.heatmap(
    cm,
    annot=annot,
    fmt='',
    cmap=sns.color_palette(["#FF6B6B", "#4ECDC4"], as_cmap=True),
    cbar=False,
    linewidths=2,
    linecolor='white',
    square=True,
    annot_kws={"fontsize":16, "weight":'bold'}
)

ax.set_xlabel('Predicted Label', fontsize=16, weight='bold')
ax.set_ylabel('True Label', fontsize=16, weight='bold')
ax.set_xticklabels(labels, fontsize=14, weight='bold')
ax.set_yticklabels(labels, fontsize=14, weight='bold', rotation=0)
ax.set_title('Confusion Matrix - Test Set', fontsize=18, weight='bold', pad=20)
plt.tight_layout()
plt.show()


print("Confusion Matrix:")
print("           Predicted Negative  Predicted Positive")
print(f"Actual Negative      {cm[0,0]}                 {cm[0,1]}")
print(f"Actual Positive      {cm[1,0]}                 {cm[1,1]}")
























