








get_ipython().getoutput("pip install isodate nltk wordcloud vaderSentiment")





import pandas as pd
import numpy as np
import time
import isodate
import seaborn as sns
import re
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import matplotlib.dates as mdates
from datetime import datetime
from scipy.stats import kruskal
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from scipy import stats
from scipy.optimize import curve_fit
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from collections import Counter
from textblob import TextBlob
from dateutil import parser
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
import matplotlib.ticker as mtick
import nltk
sns.set(style="darkgrid", color_codes=True)
nltk.download('stopwords')
nltk.download('punkt')





video_df = pd.read_csv("dataFolder/processed/cleanedDataFrame.csv")


comments_df = pd.read_csv("dataFolder/processed/cleanedComments.csv")


video_df


comments_df











video_df.isnull().any()


video_df.publishedAt.sort_values().value_counts()











video_df['engagement_rate'] = np.where(
    video_df['viewCount'] > 0, 
    (video_df['likeCount'] + video_df['commentCount']) / video_df['viewCount'],
    np.nan 
)








video_df.replace([np.inf, -np.inf], np.nan, inplace=True)
print("Engagement rate stats:")
print(f"- {video_df['engagement_rate'].isna().sum()} NaN values (zero-view videos)")
print(f"- Mean: {video_df['engagement_rate'].mean():.2%}")
print(f"- Max: {video_df['engagement_rate'].max():.2%}")
clean_df = video_df.dropna(subset=['engagement_rate']).copy()








print(clean_df[['channelTitle', 'title', 'engagement_rate']].nlargest(5, 'engagement_rate'))
max_engagement_video = clean_df.loc[clean_df['engagement_rate'].idxmax()]
print(f"Max engagement video:\nTitle: {max_engagement_video['title']}\nViews: {max_engagement_video['viewCount']}\nLikes: {max_engagement_video['likeCount']}\nComments: {max_engagement_video['commentCount']}")











print(f"Views to Engagement Ratio: {max_engagement_video['viewCount']/(max_engagement_video['likeCount'] + max_engagement_video['commentCount']):.1f}")














video_df['engagement_rate'] = video_df['engagement_rate'].replace([np.inf, -np.inf], np.nan)
clean_df = video_df.dropna(subset=['engagement_rate'])








engagement_rank = clean_df.groupby('channelTitle')['engagement_rate']\
                         .agg(['mean', 'count'])\
                         .sort_values('mean', ascending=False)\
                         .reset_index() 








plt.figure(figsize=(12,6))
ax = sns.barplot(data=engagement_rank,
                 x='mean', 
                 y='channelTitle',
                 palette=["#FF6B6B" if x == "Jordan B Peterson" else "#4ECDC4" 
                         for x in engagement_rank['channelTitle']],
                 linewidth=0.5)

for i, (mean, count) in enumerate(zip(engagement_rank['mean'], engagement_rank['count'])):
    ax.text(mean + 0.005, i, 
            f'{mean:.2%}\n(n={count})', 
            va='center', fontsize=9)








plt.title("Red Pill Engagement Leaders", fontsize=16, pad=20)
plt.xlabel("Avg. Engagement Rate", labelpad=10)
plt.ylabel("")
plt.xlim(0, engagement_rank['mean'].max() * 1.15)
ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))

plt.tight_layout




















for channel in engagement_rank['channelTitle'].head():
    chan_data = plot_df[plot_df['channelTitle'] == channel]
    avg_duration = chan_data['durationSecs'].mean() / 60
    vid_count = len(chan_data)
    print(
        f"{channel:<20} | "
        f"Avg: {avg_duration:.1f} mins | "
        f"Videos: {vid_count:,} | "
        f"Engagement: {engagement_rank[engagement_rank['channelTitle'] == channel]['mean'].values[0]:.2%}"
    )





print(f"Duration-Engagement Correlation: {clean_df['durationSecs'].corr(clean_df['engagement_rate']):.2f}")














plot_df = video_df.copy()
plot_df['engagement_rate'] = plot_df['engagement_rate'].replace([np.inf, -np.inf], np.nan)
plot_df = plot_df[plot_df['durationSecs'].between(60, 7200)]











plt.figure(figsize=(10, 6))
hexbin = plt.hexbin(
    x=plot_df['durationSecs'],
    y=plot_df['engagement_rate'],
    gridsize=30,
    cmap='viridis',
    mincnt=1, 
    extent=[0, 7200, 0, 0.2]
)





cb = plt.colorbar(hexbin, label='Number of videos')
plt.title("Optimal Video Duration for Engagement", pad=20, fontsize=14)
plt.xlabel("Duration (seconds)")
plt.ylabel("Engagement Rate")
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))





plt.axvline(x=1200, color='red', linestyle=':', linewidth=2)
plt.text(1250, 0.17, "Peak Engagement\n(20-25 min)", 
         color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.8))






plt.tight_layout()
plt.show()











optimal_mask = plot_df['durationSecs'].between(900, 1500)
print(stats.ttest_ind(
    plot_df[optimal_mask]['engagement_rate'],
    plot_df[~optimal_mask]['engagement_rate'],
    nan_policy='omit'
))








print(f"Mean engagement in optimal range: {plot_df[optimal_mask]['engagement_rate'].mean():.2%}")
print(f"Outside range: {plot_df[~optimal_mask]['engagement_rate'].mean():.2%}")








plt.bar(['Other Videos', '15-25min Videos'], 
        [4.22, 4.70], 
        color=['#4ECDC4', '#FF6B6B'])
plt.ylabel('Engagement Rate (%)')
plt.title('Engagement by Duration Range')











video_df['is_optimal_length'] = video_df['durationSecs'].between(900, 1500)
plot_df = video_df.dropna(subset=['titleLength', 'engagement_rate', 'is_optimal_length'])











plt.figure(figsize=(12, 6))
scatter = sns.scatterplot(
    data=plot_df,
    x='titleLength',
    y='engagement_rate',
    hue='is_optimal_length',
    palette={True: '#4ECDC4', False: '#FF6B6B'},
    alpha=0.4,
    s=60  
)








sns.regplot(
    data=plot_df[plot_df['is_optimal_length']],
    x='titleLength',
    y='engagement_rate',
    scatter=False,
    color='#2a7f62',  
    line_kws={'linewidth': 3, 'linestyle': '--'}
)

sns.regplot(
    data=plot_df[~plot_df['is_optimal_length']],
    x='titleLength',
    y='engagement_rate',
    scatter=False,
    color='#c43a31', 
    line_kws={'linewidth': 3, 'linestyle': ':'}
)








plt.title("Title Length vs. Engagement: Optimal vs. Non-Optimal Duration", pad=20, fontsize=14)
plt.xlabel("Title Character Length", labelpad=10)
plt.ylabel("Engagement Rate", labelpad=10)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))








legend_elements = [
    plt.Line2D([], [], color='#2a7f62', linestyle='--', linewidth=3, 
               label='Optimal Duration (15-25min) Trend'),
    plt.Line2D([], [], color='#c43a31', linestyle=':', linewidth=3,
               label='Other Durations Trend'),
    plt.Line2D([], [], marker='o', color='#4ECDC4', linestyle='None',
              markersize=8, label='Optimal Duration Videos'),
    plt.Line2D([], [], marker='o', color='#FF6B6B', linestyle='None',
              markersize=8, label='Other Duration Videos')
]











plt.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()








print("Optimal length stats:")
print(plot_df[plot_df['is_optimal_length']]['titleLength'].describe())
print("\nEngagement correlation:", plot_df[plot_df['is_optimal_length']]['titleLength'].corr(plot_df['engagement_rate']))
print("\nOther videos stats:")
print(plot_df[~plot_df['is_optimal_length']]['titleLength'].describe())
print("\nEngagement correlation:", plot_df[~plot_df['is_optimal_length']]['titleLength'].corr(plot_df['engagement_rate']))











g = sns.lmplot(
    data=plot_df,
    x='titleLength',
    y='engagement_rate',
    col='is_optimal_length',
    hue='is_optimal_length',
    palette={True: '#4ECDC4', False: '#FF6B6B'},  
    scatter_kws={'alpha': 0.4, 's': 60},
    line_kws={'linewidth': 3},  
    height=5,
    aspect=1.2,
    facet_kws={'despine': False}
)

for ax, is_optimal in zip(g.axes.flat, [True, False]):
    
    lines = ax.get_lines()
      
    if is_optimal:
        lines[0].set_linestyle('--')
        lines[0].set_color('#2a7f62') 
    else:
        lines[0].set_linestyle(':')
        lines[0].set_color('#c43a31') 

g.set_titles("Optimal Duration (15-25 min) = {col_name}")
g.set_axis_labels("Title Character Length", "Engagement Rate")

for ax in g.axes.flat:
    ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
    ax.grid(True, linestyle='--', alpha=0.2)

plt.tight_layout()
plt.show()











optimal_titles = plot_df[plot_df['is_optimal_length']]['titleLength']
print(f"Median title length for optimal videos: {optimal_titles.median():.0f} chars")
print(f"vs. other videos: {plot_df[~plot_df['is_optimal_length']]['titleLength'].median():.0f} chars")








def quadratic(x, a, b, c):
    return a*x**2 + b*x + c
popt, _ = curve_fit(quadratic, 
                   plot_df['titleLength'], 
                   plot_df['engagement_rate'])
optimal_title_len = int(-popt[1]/(2*popt[0]))
print(f"Calculated optimal title length: {optimal_title_len} characters")

















plt.figure(figsize=(12, 7))








ax = sns.boxplot(
    data=video_df,
    x='publishDayName',
    y='engagement_rate',
    order=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],
    palette="pastel",
    width=0.6
)








def add_stats(ax, df):
    stats = df.groupby('publishDayName')['engagement_rate'].describe()
    for i, day in enumerate(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']):
        day_stats = stats.loc[day]
        
        ax.text(i, day_stats['50%'], f'Med: {day_stats["50%"]:.1%}',
                ha='center', va='center', fontsize=9, color='black', 
                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))
        
        ax.plot(i, day_stats['min'], 'v', color='darkgray', markersize=6)
        ax.plot(i, day_stats['max'], '^', color='darkgray', markersize=6)

        ax.plot([i-0.3, i+0.3], [day_stats['mean'], day_stats['mean']], 
                color='red', linestyle='--', linewidth=1)

add_stats(ax, video_df)








plt.title("Engagement Rate by Day of Week\n(Triangles show min/max, dashed line = mean)", 
          fontsize=14, pad=15)
plt.xlabel("Day of Week", labelpad=12)
plt.ylabel("Engagement Rate", labelpad=12)








plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
plt.xticks(rotation=45)








legend_elements = [
    plt.Line2D([0], [0], marker='v', color='darkgray', label='Min', markersize=8),
    plt.Line2D([0], [0], marker='^', color='darkgray', label='Max', markersize=8),
    plt.Line2D([0], [0], color='red', linestyle='--', label='Mean')
]
plt.legend(handles=legend_elements, loc='upper right')











plt.tight_layout()
plt.show()





day_stats = (video_df.groupby('publishDayName')['engagement_rate']
             .describe(percentiles=[.25, .5, .75])
             .reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday']))

correlation_stats = video_df.groupby('publishDayName')['engagement_rate'].agg(['mean', 'median', 'std', 'count'])

print("=== Detailed Engagement Statistics by Day ===")
display(day_stats[['count', 'mean', '50%', 'min', '25%', '75%', 'max']].style
        .format('{:.2%}')
        .background_gradient(cmap='Blues', subset=['mean', '50%']))











clean_stats = day_stats.replace([np.inf, -np.inf], np.nan).dropna()

plt.figure(figsize=(10, 6))

plt.plot(clean_stats.index, clean_stats['mean'], 
         marker='o', markersize=8, label='Average Engagement', 
         color='#2b8cbe', linewidth=2)

plt.fill_between(range(len(clean_stats)),
                 clean_stats['25%'],
                 clean_stats['75%'],
                 alpha=0.2, color='#a6bddb', label='IQR (25%-75%)')

for i, day in enumerate(clean_stats.index):
    plt.text(i, clean_stats.loc[day, 'mean'] + 0.003, 
             f"{clean_stats.loc[day, 'mean']:.1%}",
             ha='center', va='bottom', fontsize=9)

plt.title('Daily Engagement Rate Trends', pad=15, fontsize=14)
plt.xlabel('Day of Week', labelpad=10)
plt.ylabel('Engagement Rate', labelpad=10)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))
plt.xticks(range(len(clean_stats)), clean_stats.index, rotation=45)

plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

plt.tight_layout()
plt.show()








top_10 = video_df[video_df['engagement_rate'] > video_df['engagement_rate'].quantile(0.9)]
top_day_dist = (top_10['publishDayName']
                .value_counts(normalize=True)
                .reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])
                .to_frame('Percentage')
                )

print("\n=== Where Top 10% Engaged Videos Appear ===")

def format_percentage(x):
    if isinstance(x, (int, float)):
        return f"{x:.1%}"
    return x

display(top_day_dist.style.format({'Percentage': format_percentage}))














video_df['publishedAt'] = pd.to_datetime(video_df['publishedAt'])








video_df['publishHour'] = video_df['publishedAt'].dt.hour
video_df['publishDayName'] = (video_df['publishedAt']
                             .dt.day_name()
                             .astype('category')
                             .cat.reorder_categories(
                                 ['Monday','Tuesday','Wednesday',
                                  'Thursday','Friday','Saturday','Sunday'],
                                 ordered=True))








plot_df = video_df.replace([np.inf, -np.inf], np.nan).dropna(subset=['engagement_rate']).copy()








heatmap_data = (plot_df.groupby(['publishDayName', 'publishHour'], observed=True)
                ['engagement_rate']
                .mean()
                .unstack())








plt.figure(figsize=(14, 6))
ax = sns.heatmap(
    data=heatmap_data,
    cmap='YlOrRd',
    annot=False,
    fmt='.1%', 
    linewidths=0.3,
    cbar_kws={'label': 'Engagement Rate'}
)








plt.title("Optimal Posting Times by Day/Hour", pad=20, fontsize=14)
plt.xlabel("Hour of Day (24h)", fontsize=12)
plt.ylabel("", fontsize=12)
plt.xticks(rotation=0)
plt.yticks(rotation=0)








max_val = heatmap_data.max().max()
for (day, hour), val in heatmap_data.stack().items():
    if val == max_val:
        ax.add_patch(plt.Rectangle(
            (hour, list(heatmap_data.index).index(day)), 
            1, 1, 
            fill=False, 
            edgecolor='blue',
            lw=3
        ))
        ax.text(hour+0.5, list(heatmap_data.index).index(day)+0.5, 
                "BEST", 
                ha='center', 
                va='center', 
                color='blue',
                fontweight='bold',
                fontsize=10)











plt.tight_layout()
plt.show()





styled_data = heatmap_data * 100 
styled_data = styled_data.style.format("{:.2f}%") 
styled_data = styled_data.set_caption("Average Engagement Rate by Day and Hour")  
styled_data = styled_data.background_gradient(cmap="YlOrRd")
styled_data











analyzer = SentimentIntensityAnalyzer()

def get_sentiment(text):
    if pd.notnull(text):
        scores = analyzer.polarity_scores(text)
        compound = scores['compound'] 
        if compound <= -0.1:
            return "Negative"
        elif compound >= 0.1:
            return "Positive"
        else: 
            return "Neutral"
    return None








video_df['sentiment_bin'] = video_df['title'].apply(get_sentiment)








plot_df = video_df.dropna(subset=['sentiment_bin', 'engagement_rate'])








plt.figure(figsize=(10, 6))
sns.boxplot(
    data=plot_df,
    x='sentiment_bin',
    y='engagement_rate',
    palette='coolwarm',
    showfliers=False,
    width=0.6,
    order=["Negative", "Neutral", "Positive"], 
)








plt.title("Impact of Title Sentiment on Engagement", pad=20, fontsize=14)
plt.xlabel("Title Sentiment", fontsize=12)
plt.ylabel("Engagement Rate", fontsize=12)
plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1.0))











plt.tight_layout()
plt.show()
print(plot_df['sentiment_bin'].value_counts().sort_index())





print("Descriptive Statistics for Engagement Rates by Sentiment Bin:")
print(plot_df.groupby('sentiment_bin')['engagement_rate'].describe())

















for sentiment in ["Negative", "Neutral", "Positive"]:
    print(f"\nSample Titles for {sentiment}:")
    print(plot_df[plot_df['sentiment_bin'] == sentiment]['title'].sample(3, random_state=42).tolist())

















video_df['retention_ratio'] = video_df['viewCount'] * 2.5 / video_df['durationSecs']








video_df = video_df[(video_df['durationSecs'] > 0) & (video_df['retention_ratio'] < 1000)]








sns.set(style="whitegrid")  
plt.figure(figsize=(12, 8))  








sns.scatterplot(
    data=video_df,
    x='durationSecs',
    y='retention_ratio',
    hue='channelTitle', 
    palette='tab10', 
    alpha=0.7
)








plt.title("Scatter Plot of Video Duration vs Retention Ratio by Channel", fontsize=16, pad=20)
plt.xlabel("Video Duration (seconds)", fontsize=12)
plt.ylabel("Retention Ratio", fontsize=12)
plt.legend(title='Channel', bbox_to_anchor=(1.05, 1), loc='upper left')  # Move legend outside the plot











plt.tight_layout()
plt.show()








sns.set(style="whitegrid") 

facet = sns.FacetGrid(
    video_df,
    col="channelTitle",  
    col_wrap=3,          
    height=4,            
    aspect=1.2           
)

facet.map(
    sns.scatterplot,
    "durationSecs",
    "retention_ratio",
    alpha=0.7,
    color="blue"
)

for ax in facet.axes.flat:
    ax.axhline(y=1, color='red', linestyle='--', linewidth=1)

facet.set_titles("{col_name}") 
facet.set_axis_labels("Video Duration (seconds)", "Retention Ratio")
facet.fig.suptitle("Scatter Plots of Video Duration vs Retention Ratio by Channel", fontsize=16, y=1.03)

plt.tight_layout()
plt.show()








print("Overall Statistical Summary:")
print(video_df[['durationSecs', 'retention_ratio']].describe())
print("\nPer-Channel Summary:")
channel_summary = video_df.groupby('channelTitle').agg({
    'durationSecs': ['mean', 'median', 'std'],
    'retention_ratio': ['mean', 'median', 'std']
})
print(channel_summary)

top_retention_videos = video_df.nlargest(5, 'retention_ratio')
print("\nTop 5 Videos with Highest Retention Ratio:")
print(top_retention_videos[['channelTitle', 'durationSecs', 'retention_ratio']])

low_retention_videos = video_df.nsmallest(5, 'retention_ratio')
print("\nTop 5 Videos with Lowest Retention Ratio:")
print(low_retention_videos[['channelTitle', 'durationSecs', 'retention_ratio']])

print("\nRetention Ratio Ranges by Channel:")
retention_ranges = video_df.groupby('channelTitle')['retention_ratio'].apply(
    lambda x: f"{x.min()} to {x.max()}"
)
print(retention_ranges)














video_df['publishedAt'] = pd.to_datetime(video_df['publishedAt'], errors='coerce')

video_df = video_df.dropna(subset=['publishedAt'])

yearly_growth = video_df.groupby(['channelTitle', pd.Grouper(key='publishedAt', freq='Y')])['viewCount'].sum().unstack()

non_zero_yearly_growth = yearly_growth[(yearly_growth.T != 0).any()]

non_zero_yearly_growth.T[['FreshandFit', 'Better Bachelor', 'Jordan B Peterson']].plot(
    figsize=(12, 6),
    style=['-o', '--s', ':^'],
    title="Yearly View Growth Trends (Top 3 Channels)"
)
plt.grid(visible=True, which='both', linestyle='--', linewidth=0.5)
plt.xlabel("Year")
plt.ylabel("Total View Count")
plt.legend(title="Channel")
plt.tight_layout()
plt.show()











top_3_channels = ['FreshandFit', 'Better Bachelor', 'Jordan B Peterson']

data_top_3 = non_zero_yearly_growth.loc[top_3_channels]

print("Yearly View Growth Data (Top 3 Channels):")
print(data_top_3)

for channel in top_3_channels:
    channel_data = data_top_3.loc[channel]
    
    peak_year = channel_data.idxmax() 
    peak_value = channel_data.max()    
    low_year = channel_data.idxmin()  
    low_value = channel_data.min()  

    print(f"\nChannel: {channel}")
    print(f"  Peak Year: {peak_year.year}, Views: {peak_value}")
    print(f"  Lowest Year: {low_year.year}, Views: {low_value}")








key_months = {
    "FreshandFit": [(2021, 8), (2020, 1)], 
    "Better Bachelor": [(2020, 6), (2019, 1)],
    "Jordan B Peterson": [(2022, 7), (2014, 1)]
}
for channel, months in key_months.items():
    print(f"--- Top Videos for {channel} ---\n")
    for year, month in months:
        videos_in_month = video_df[
            (video_df['publishedAt'].dt.year == year) &
            (video_df['publishedAt'].dt.month == month) &
            (video_df['channelTitle'] == channel)
        ]

        top_videos = videos_in_month.sort_values(by='viewCount', ascending=False)
        month_name = datetime(year, month, 1).strftime('%B %Y')
        print(f"Key Date: {month_name}")
        if not top_videos.empty:
            print("Top Video Titles:")
            for title in top_videos['title']:
                print(f"  - {title}")
        else:
            print("No videos found.")
        print("\n")









# 1. Clean and extract words (fixed version)
all_words = []
for title in video_df[video_df['engagement_rate'] > 0.1]['title']:
    # Remove punctuation and split
    words = re.findall(r'\b[a-z]+\b', title.lower())  # Cleans better than str.replace
    all_words.extend(words)

# 2. Count and filter
word_counts = Counter(all_words)
stopwords = set(['the', 'and', 'to', 'of', 'a', 'in', 'is', 'it', 'that', 'this'])
top_words = {word: count for word, count in word_counts.items() 
             if word not in stopwords and len(word) > 3}  # Filter short words

# 3. Get top 15
top_15 = Counter(top_words).most_common(15)

# 4. Plot with improved formatting
plt.figure(figsize=(10, 6))
sns.barplot(
    x=[count for word, count in top_15],
    y=[word for word, count in top_15],
    palette='magma_r',
    edgecolor='black'
)

# 5. Professional annotations
plt.title("Top 15 Title Words in High-Engagement Videos (ER > 10%)", pad=15)
plt.xlabel("Frequency Count")
plt.ylabel("")
for i, (word, count) in enumerate(top_15):
    plt.text(count + 5, i, f"{count}", va='center')

plt.tight_layout()
plt.show()





# 1. Sample and merge data (ensure 'video_id' exists in both DataFrames)
comment_sample = comments_df.sample(1000, random_state=42).copy()
merged_data = comment_sample.merge(
    video_df[['video_id', 'viewCount']],  # Make sure video_df has 'video_id'
    on='video_id',
    how='left'
)

# 2. Calculate sentiment ON THE MERGED DATA
merged_data['sentiment'] = merged_data['comment'].apply(
    lambda x: TextBlob(str(x)).sentiment.polarity if pd.notnull(x) else np.nan
)

# 3. Filter valid data
plot_data = merged_data[
    (merged_data['viewCount'] > 0) & 
    (merged_data['sentiment'].notna())
].copy()

# 4. Create visualization
if len(plot_data) > 0:
    plt.figure(figsize=(12, 6))
    hexbin = plt.hexbin(
        x=plot_data['sentiment'],
        y=np.log10(plot_data['viewCount'] + 1),  # +1 to avoid log(0)
        gridsize=20,
        cmap='Reds',
        mincnt=1,
        extent=[-1, 1, 0, np.log10(plot_data['viewCount'].max() + 1)]
    )
    
    # Formatting
    plt.title(f"Comment Sentiment vs. Video Popularity (n={len(plot_data)})", pad=20)
    plt.xlabel("Comment Sentiment (Negative to Positive)")
    plt.ylabel("Log10(View Count + 1)")
    cb = plt.colorbar(hexbin, label='Number of Comments')
    plt.axvline(0, color='black', linestyle=':', alpha=0.5)
    
    plt.tight_layout()
    plt.show()
else:
    print("Warning: No valid data points after filtering!")
    print("Check if: 1) Comments merged correctly, 2) View counts exist, 3) Sentiment was calculated")





video_df['publishHour_sin'] = np.sin(2 * np.pi * video_df['publishHour']/24)
video_df['publishHour_cos'] = np.cos(2 * np.pi * video_df['publishHour']/24)

# 2. Feature engineering (verify columns exist)
required_features = ['durationSecs', 'title_sentiment', 'tagCount', 
                    'publishHour_sin', 'publishHour_cos']
assert all(feat in video_df.columns for feat in required_features), \
       f"Missing columns: {set(required_features) - set(video_df.columns)}"

X = video_df[required_features]
y = video_df['engagement_rate']

# 3. Handle missing values and align X and y
# Create a mask of rows where neither X nor y has NA values
mask = X.notna().all(axis=1) & y.notna()

# Apply the mask to both X and y
X_clean = X[mask]
y_clean = y[mask]

# 4. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_clean, 
    y_clean, 
    test_size=0.2, 
    random_state=42
)

# 5. Train the model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 6. Evaluate model performance
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)
print(f"Training R²: {train_score:.3f}")
print(f"Test R²: {test_score:.3f}")

# 7. Feature importance analysis
importances = pd.DataFrame({
    'feature': X_train.columns,
    'importance': model.feature_importances_,
    'std': np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)
}).sort_values('importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(
    data=importances,
    x='importance',
    y='feature',
    xerr=importances['std'],
    palette='viridis'
)
plt.title("Feature Importance with Standard Deviation", pad=20)
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

# Optional: Show the feature importance dataframe
print("\nFeature Importances:")
print(importances)





stop_words = set(stopwords.words('english'))
comments_df['comments_no_stopwords'] = comments_df['comment'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])

all_words = list([a for b in comments_df['comments_no_stopwords'].tolist() for a in b])
all_words_str = ' '.join(all_words)


wordcloud = WordCloud(width = 2000, height = 1000, random_state=1, background_color='black',
                      colormap='viridis', collocations=False).generate(all_words_str)
plot_cloud(wordcloud)



